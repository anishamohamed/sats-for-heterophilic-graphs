
The following have been reloaded with a version change:
  1) gcc/4.8.5 => gcc/8.2.0

/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/optuna/samplers/_tpe/sampler.py:314: ExperimentalWarning: ``constant_liar`` option is an experimental feature. The interface can change in the future.
  warnings.warn(
[I 2023-12-11 16:38:48,467] A new study created in memory with name: no-name-5e4d2805-3bad-436c-8856-9bcb08260c02
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:43: attribute 'lr_scheduler' removed from hparams because it cannot be pickled
wandb: Currently logged in as: anishamohamed00. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in ./wandb/run-20231211_163938-tjkto47a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-fire-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/anishamohamed00/g2_sat_ZINC
wandb: üöÄ View run at https://wandb.ai/anishamohamed00/g2_sat_ZINC/runs/tjkto47a
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type             | Params
-----------------------------------------------
0 | model     | GraphTransformer | 420 K 
1 | criterion | L1Loss           | 0     
-----------------------------------------------
420 K     Trainable params
0         Non-trainable params
420 K     Total params
1.681     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
