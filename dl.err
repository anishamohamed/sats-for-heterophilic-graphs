
The following have been reloaded with a version change:
  1) gcc/4.8.5 => gcc/8.2.0

/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/optuna/samplers/_tpe/sampler.py:314: ExperimentalWarning: ``constant_liar`` option is an experimental feature. The interface can change in the future.
  warnings.warn(
[I 2023-12-11 16:03:40,578] A new study created in memory with name: no-name-0da729cb-e735-4e6c-bc5f-89d6f230197d
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:43: attribute 'lr_scheduler' removed from hparams because it cannot be pickled
wandb: Currently logged in as: anishamohamed00. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in ./wandb/run-20231211_160433-pz7b8l3j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-snowflake-15
wandb: ⭐️ View project at https://wandb.ai/anishamohamed00/g2_sat_ZINC
wandb: 🚀 View run at https://wandb.ai/anishamohamed00/g2_sat_ZINC/runs/pz7b8l3j
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type             | Params
-----------------------------------------------
0 | model     | GraphTransformer | 1.5 M 
1 | criterion | L1Loss           | 0     
-----------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.037     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:               epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb:           test/loss ▁
wandb:          train/loss █▇▅▃▂▂▂▁▁▁▁▁
wandb: trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███
wandb:            val/loss █▆▄▂▂▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 12
wandb:           test/loss 0.61664
wandb:          train/loss 0.50674
wandb: trainer/global_step 948
wandb:            val/loss 0.57994
wandb: 
wandb: 🚀 View run volcanic-snowflake-15 at: https://wandb.ai/anishamohamed00/g2_sat_ZINC/runs/pz7b8l3j
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231211_160433-pz7b8l3j/logs
[I 2023-12-11 16:12:04,667] Trial 0 finished with value: 0.6166448593139648 and parameters: {'num_layers': 4, 'gnn_type': 'gcn', 'k_hop': 32, 'gradient_gating_p': 0.0}. Best is trial 0 with value: 0.6166448593139648.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:43: attribute 'lr_scheduler' removed from hparams because it cannot be pickled
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in ./wandb/run-20231211_161236-w0k2ogho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-star-16
wandb: ⭐️ View project at https://wandb.ai/anishamohamed00/g2_sat_ZINC
wandb: 🚀 View run at https://wandb.ai/anishamohamed00/g2_sat_ZINC/runs/w0k2ogho
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type             | Params
-----------------------------------------------
0 | model     | GraphTransformer | 739 K 
1 | criterion | L1Loss           | 0     
-----------------------------------------------
739 K     Trainable params
0         Non-trainable params
739 K     Total params
2.958     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
/cluster/home/amohame/miniconda/envs/g2-sat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.
